distod {

  // actor system name used in remoting
  system-name = "distod"
  // role of the actor system in the cluster (either leader or follower)
  system-role = "leader"

  input {
    // filepath pointing to a dataset that is read and analyzed for ODs
    path = "data/flight/flight_500_28c.csv"
//    path = "data/flights_20_500k.csv"
    // if the input file has a header
    has-header = yes

    // restrict number of columns
//    max-columns = 10
    // restrict number of rows
//    max-rows = 1000

  }

  // filepath where the result is stored
  output-file = "data/results.txt"
  // should the results also be printed to console?
  output-to-console = off
  // number of dependencies that are grouped into one message to be sent to the result collector by the proxies
  result-batch-size = 50

  // network configuration of current node
  host = "127.0.0.1"
  port = 7878

  // remote address of leader node
  leader-host = ${distod.host}
  leader-port = ${distod.port}

  // limits the number of parallelism used by this node
  // this includes the parallel master components, sidechannel deserialization, data IO, the number of
  // workers, the number of partition generators, and all other parallel work
  // the number of used actors per category will never be greater than the number of available cores
  max-parallelism = 64

  // limit the number of workers spawned by this node (candidate checking workers & partition generation workers)
  // actual: #workers = min(#cores, max-workers)
  // this does also limit the max number of threads for the cpu-bound-task-dispatcher
  max-workers = ${distod.max-parallelism}

  // Sets the number of check jobs a worker can process in parallel. This helps with hiding some latency introduced by
  // the "centralized" (central for each node) partition management and generation, because more partitions are
  // requested concurrently and it is less likely that all workers wait for one slow partition generation task.
  concurrent-worker-jobs = 4

  // Give a hint to DISTOD, how big it can expect the cluster to grow. Use higher values over lower ones, because it
  // limits the message stash sizes and therefore also the number of supported nodes. If the expected-node-count is set
  // too small, the master components may fail with a `StashOverflowException`. In this case you must increase this value.
  expected-node-count = 20

  // Set the threshold, on which partitions are generated directly from the singleton partitions instead of incrementally
  // from their predecessors (recursively). When the product job chain for a specific partition is smaller than this
  // threshold, the partition is generated using the incremental approach [ABCD = ABC * (AB * (A * D))]. When the chain
  // would be greater than this threshold, the partition is generated directly from the singleton partitions with one
  // big product operation [ABCD = A * B * C * D].
  direct-partition-product-threshold = 15

  // This setting can be used to disable the partition cache completely. This means that all partitions are computed on
  // demand for every request. This also disables the partition compaction, because there are no partitions to be
  // removed when a compaction is triggered. Per default, the partition cache is enabled, because it improves
  // performance greately.
  enable-partition-cache = yes

  // Partition compaction removes old partitions from the partition manager to free up memory and increase lookup speed
  // It can safely be enabled, because singleton partitions are never removed from the partition manager, so that all
  // other partitions can be generated from them. It may impact performance when it is triggered too often, so that a
  // partition is generated multiple times.
  partition-compaction {
    enabled = yes
    // interval (in seconds or minutes) for removing old partitions from the partition manager
    interval = 40s
  }

  // DISTOD monitors the JVM to adapt its algorithms to changing conditions. Currently only the heap is monitored.
  monitoring {
    // sampling rate
    interval = 1s

    // If memory usage reaches the 'heap-eviction-threshold', DISTOD tries to free up temporary data structures (which
    // increase the speed, but are not necessary) to allow running the algorithm with lower memory boundaries. Please
    // specify the threshold in percent. Be careful! If it is 100, heap eviction is disabled.
    // This value should match the value set with -XX:G1ReservePercent=(1 - heap-eviction-threshold). Default of
    // -XX:G1ReservePercent is 10%.
    heap-eviction-threshold = 90

    statistics-log-interval = 30s

    statistics-log-level = "DEBUG"
  }

  // DISTOD is able to prune ODs semantically.
  // Attention: turning on any of these options makes the results of DISTOD incomplete, but it drastically decreases the
  // number of results and therefore also DISTOD's runtime
  pruning {
    // This limits the number of attributes that can be involved in an OD and greatly reduces the number of outputted
    // ODs. A limit of 3 would give bidirectional ODs with 3 distinct attributes of the forms:
    // - {}: [] â†¦ A, {}: A ~ BðŸ¡™
    // - {A}: [] â†¦ B, {A}: B ~ CðŸ¡™
    // - {A,B}: [] â†¦ C
    // This must be equal to or greater than 1. If set to 1 only FDs of the form {}: [] â†¦ A (constant columns) will be
    // found!
//    od-size-limit = 5

    // Interestingness pruning uses the coverage of ODs (how many tuples contribute to the OD) to determine an
    // interestingness score. You can set a threshold for it, so only ODs with a higher score are considered.
//    interestingness-threshold = 10000000
  }

  // Dispatcher used for CPU-bound task processing to free up default dispatcher from this heavy load.
  // A step towards making nodes in a cluster responsive (allow scheduled cluster heartbeats and custom gossip)
  cpu-bound-tasks-dispatcher {
    type = Dispatcher
    executor = "thread-pool-executor"
    // Configuration for the thread pool
    thread-pool-executor {
      // Keep alive time for threads
      keep-alive-time = 20s

      // core pool: number of threads spawned when queue is not full yet (we limited the queue to be of size 1)
      core-pool-size-min = 1
      core-pool-size-factor = 0.5 // ceil(available processors * factor)
      core-pool-size-max = 32

      // max number of threads (thread capacitiy)
      max-pool-size-min = 1
      max-pool-size-factor = 1.0
      max-pool-size-max = ${distod.max-workers} // 64

      // Specifies the bounded capacity of the task queue (< 1 == unbounded)
      task-queue-size = 1
      task-queue-type = "linked"
    }
    // Throughput defines the maximum number of messages to be
    // processed per actor before the thread jumps to the next actor.
    // Set to 1 for as fair as possible.
    throughput = 1
  }

  master-pinned-dispatcher {
    executor = "thread-pool-executor"
    type = PinnedDispatcher
    thread-pool-executor {
      fixed-pool-size = 1
      allow-core-timeout = off
    }
    throughput = 1
  }
}

akka {

  // use SLF4J logger
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  loglevel = "INFO"
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
  log-config-on-start = off

  actor {
    provider = cluster

    // In addition to the default there is akka.actor.StoppingSupervisorStrategy.
    guardian-supervisor-strategy = "akka.actor.DefaultSupervisorStrategy"

    serializers {
      java = "akka.serialization.JavaSerializer"
      bytes = "akka.serialization.ByteArraySerializer"
      primitive-long = "akka.serialization.LongSerializer"
      primitive-int = "akka.serialization.IntSerializer"
      primitive-string = "akka.serialization.StringSerializer"
      primitive-bytestring = "akka.serialization.ByteStringSerializer"
      primitive-boolean = "akka.serialization.BooleanSerializer"
//      kryo = "com.twitter.chill.akka.AkkaSerializer"
      jackson-json = "akka.serialization.jackson.JacksonJsonSerializer"
      jackson-cbor = "akka.serialization.jackson.JacksonCborSerializer"
    }

    serialization-bindings {
      "[B" = bytes
      "java.io.Serializable" = none
//      "scala.Serializable" = none
      // Java Serializer is by default used for exceptions and will by default
      // not be allowed to be serialized, but in certain cases they are replaced
      // by `akka.remote.serialization.ThrowableNotSerializableException` if
      // no specific serializer has been defined:
      // - when wrapped in `akka.actor.Status.Failure` for ask replies
      // - when wrapped in system messages for exceptions from remote deployed child actors
      "java.lang.Throwable" = none

      "java.lang.String" = primitive-string
      "akka.util.ByteString$ByteString1C" = primitive-bytestring
      "akka.util.ByteString$ByteString1" = primitive-bytestring
      "akka.util.ByteString$ByteStrings" = primitive-bytestring
      "java.lang.Long" = primitive-long
      "scala.Long" = primitive-long
      "java.lang.Integer" = primitive-int
      "scala.Int" = primitive-int
      "java.lang.Boolean" = primitive-boolean
      "scala.Boolean" = primitive-boolean

      "com.github.codelionx.distod.Serialization$JsonSerializable" = jackson-json
      "com.github.codelionx.distod.Serialization$CborSerializable" = jackson-cbor

//      "akka.stream.impl.streamref.StreamRefsProtocol$CumulativeDemand" = java
//      "akka.stream.impl.streamref.StreamRefsProtocol$OnSubscribeHandshake" = java
//      "akka.stream.impl.streamref.StreamRefsProtocol$RemoteStreamFailure" = java
//      "akka.stream.impl.streamref.StreamRefsProtocol$SequencedOnNext" = java
//      "akka.stream.impl.streamref.StreamRefsProtocol$RemoteStreamCompleted" = java
    }

    // force serialization even for local messages
//    serialize-messages = on

    allow-java-serialization = off
    warn-about-java-serializer-usage = on
  }

  coordinated-shutdown {
    terminate-actor-system = on

    run-by-jvm-shutdown-hook = on
    run-by-actor-system-terminate = on

    // hard exit of JVM (using System.exit)
    exit-jvm = on

    phases: {
      stop-workers {
        timeout = 1m
      }
      before-service-unbind {
        depends-on = [stop-workers]
      }
    }
  }

  remote {
    artery {
      enabled = on
      transport = tcp // prefer tcp over aeron-udp
      canonical.hostname = ${distod.host}
      canonical.port = ${distod.port}

      // Bind to a NIC at a different address (useful for Docker, NATs, ...)
      // If canonical and bind addresses are different, then network configuration that relays communications from
      // canonical to bind addresses is expected.
      bind.hostname = ${distod.host}
      bind.port = ${distod.port}

      // large message channel
      large-message-destinations = [
        "/user/partition-replicator/**"
      ]
      advanced {
        maximum-large-frame-size = 2 MiB
        large-buffer-pool-size = 32
        outbound-large-message-queue-size = 256
      }
    }

    log-received-messages = off
    log-sent-messages = off

    watch-failure-detector {
      implementation-class = "akka.remote.PhiAccrualFailureDetector"
      heartbeat-interval = 2s
      threshold = 12.0
      max-sample-size = 1000
      min-std-deviation = 100ms
      acceptable-heartbeat-pause = 10s
      expected-response-after = 2s
    }
  }

  cluster {
    min-nr-of-members = 1

    role {
      leader.min-nr-of-members = 1
      follower.min-nr-of-members = 0
    }
    roles = [
      ${distod.system-role}
    ]

    seed-nodes = [
      "akka://"${distod.system-name}"@"${distod.leader-host}":"${distod.leader-port}
    ]
    seed-node-timeout = 5s
    // Disable join retry by specifying "off"
    retry-unsuccessful-join-after = 10s

    // Pluggable support for downing of nodes in the cluster.
    // If this setting is left empty the `NoDowning` provider is used and no automatic downing will be performed.
    downing-provider-class = ""

    // This will terminate the ActorSystem when the cluster extension is shutdown
    run-coordinated-shutdown-when-down = on

    // how often should the node send out gossip information?
    gossip-interval = 1s

    // discard incoming gossip messages if not handled within this duration
    gossip-time-to-live = 2s

    // how often should the leader perform maintenance tasks?
    leader-actions-interval = 1s

    failure-detector {
      implementation-class = "akka.remote.PhiAccrualFailureDetector"
      heartbeat-interval = 2s
      threshold = 12.0
      max-sample-size = 1000
      min-std-deviation = 100ms
      acceptable-heartbeat-pause = 10s
      monitored-by-nr-of-members = 5 // default: 5
      expected-response-after = 2s
    }
  }
}
