include classpath("application")

distod {

  output-file = "results.txt"
  output-to-console = off

  result-batch-size = 100

  leader-host = "odin01"
  leader-port = 7878
}

////////////////////////////////////////////////////
// Dispatcher configurations for "exp9-dispatchers"
////////////////////////////////////////////////////
// Otheverwrite these keys with one of the dispatcher configuration names below to use them:
// - distod.cpu-bound-tasks-dispatcher
// - distod.master-pinned-dispatcher

master-dispatcher {
    executor = "thread-pool-executor"
    type = PinnedDispatcher
    thread-pool-executor {
        fixed-pool-size = 1
        allow-core-timeout = off
    }
    throughput = 1
}

thread-pool-dispatcher {
    type = Dispatcher
    executor = "thread-pool-executor"
    // Configuration for the thread pool
    thread-pool-executor {
        // Keep alive time for threads
        keep-alive-time = 20s

        // core pool: number of threads spawned when queue is not full yet (we limited the queue to be of size 1)
        core-pool-size-min = 1
        core-pool-size-factor = 0.5 // ceil(available processors * factor)
        core-pool-size-max = 32

        // max number of threads (thread capacitiy)
        max-pool-size-min = 1
        max-pool-size-factor = 1.0
        max-pool-size-max = ${distod.max-workers} // 64

        // Specifies the bounded capacity of the task queue (< 1 == unbounded)
        task-queue-size = 1
        task-queue-type = "linked"
    }
    // Throughput defines the maximum number of messages to be
    // processed per actor before the thread jumps to the next actor.
    // Set to 1 for as fair as possible.
    throughput = 1
}

worker-affinity-dispatcher {
    type = Dispatcher
    executor = "affinity-pool-executor"

    affinity-pool-executor {
        // parallelism
        parallelism-min = 4
        parallelism-factor = 0.8 // ceil(available processors * factor)
        parallelism-max = 64

        // Each worker in the pool uses a separate bounded MPSC queue. Whenever an attempt to enqueue a task is made
        // and the queue does not have capacity to accommodate the task, the rejection handler created by the
        // rejection handler specified in "rejection-handler" is invoked.
        task-queue-size = 512
        rejection-handler = "akka.dispatch.affinity.ThrowOnOverflowRejectionHandler"

        // Level of CPU time used, on a scale between 1 and 10, during backoff/idle.
        // Level 1 strongly prefer low CPU consumption over low latency.
        // Level 10 strongly prefer low latency over low CPU consumption.
        idle-cpu-level = 5


        // When using the "akka.dispatch.affinity.FairDistributionHashCache" queue selector
        // internally the AffinityPool uses two methods to determine which task
        // queue to allocate a Runnable to:
        // - map based - maintains a round robin counter and a map of Runnable
        // hashcodes to queues that they have been associated with. This ensures
        // maximum fairness in terms of work distribution, meaning that each worker
        // will get approximately equal amount of mailboxes to execute. This is suitable
        // in cases where we have a small number of actors that will be scheduled on
        // the pool and we want to ensure the maximum possible utilization of the
        // available threads.
        // - hash based - the task - queue in which the runnable should go is determined
        // by using an uniformly distributed int to int hash function which uses the
        // hash code of the Runnable as an input. This is preferred in situations where we
        // have enough number of distinct actors to ensure statistically uniform
        // distribution of work across threads or we are ready to sacrifice the
        // former for the added benefit of avoiding map look-ups.
        queue-selector = "akka.dispatch.affinity.FairDistributionHashCache"
        fair-work-distribution {
          // The value serves as a threshold which determines the point at which the
          // pool switches from the first to the second work distribution schemes.
          // For example, if the value is set to 128, the pool can observe up to
          // 128 unique actors and schedule their mailboxes using the map based
          // approach. Once this number is reached the pool switches to hash based
          // task distribution mode. If the value is set to 0, the map based
          // work distribution approach is disabled and only the hash based is
          // used irrespective of the number of unique actors. Valid range is
          // 0 to 2048 (inclusive)
          threshold = 128
        }
    }
}
